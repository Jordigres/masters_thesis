{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 mobility restriction graphs characterization using TRIPS\n",
    "\n",
    "\n",
    "### Code contributors:\n",
    "- Jordi Grau <jordi.grau@eurecat.org>\n",
    "\n",
    "### Content\n",
    "This notebook finds differences in Catalunya's mobilty across the previously defined COVID-19 phases using network science techniques. In this notebook the focus is in the numer trips graphs.   \n",
    "\n",
    "### Contents\n",
    "\n",
    "1. [Data Loading](#data)   \n",
    "    1.1. [Daily fluxes queries](#df)   \n",
    "    1.2. [Low trips edges removal](#removal)   \n",
    "    1.3. [Region geometries, centroids and geographical distances](#geography-data)\n",
    "\n",
    "\n",
    "2. [Graphs creation for each COVID-19 phase](#graphs)\n",
    "\n",
    "\n",
    "3. [Graph analysis](#graph-analysis)  \n",
    "    3.1. [Mobility map](#mobility-map)   \n",
    "    3.2. [Degree centrality and density](#degree-density)      \n",
    "    3.3. [In-degree and out-degree centralities](#in-out)   \n",
    "    3.4. [Distance probability](#distance-prob)   \n",
    "    3.5. [Shortest Path and their Lengths $L_{ij}$](#shortest-paths)   \n",
    "    3.6. [Shortest Path trees](#shortest-paths-trees)   \n",
    "    3.7. [Closeness centrality](#closness)   \n",
    "    3.8. [Betweenness centraliy](#betweenness)   \n",
    "    3.9. [Transitivity and Clustering Coefficient ($C_{ij}$)](#clustering)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "from os.path import exists\n",
    "import numpy as np\n",
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "from sklearn import linear_model\n",
    "\n",
    "from src.mobility_context_and_queries import *\n",
    "\n",
    "# URL to save processed dataframes and graphs\n",
    "save_url = '../../jupyter/Observatori/data/processed/graphs and metrics/trips/'\n",
    "\n",
    "# COVID-19 phases\n",
    "phases_names = [list(i.keys())[0] for i in phases_list]\n",
    "phases_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading <a class=\"anchor\" id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Daily fluxes queries <a class=\"anchor\" id=\"df\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DAILY FLUX query between 2 dates of trips from or to Catalunya \n",
    "date1 = phases_list[0]['precovid']['start']\n",
    "date2 = phases_list[9]['nadal']['end']\n",
    "province_group = list(location_info.items())\n",
    "\n",
    "df_flux_daily = query_raw_data_or_trips_or_flux_matrix_between_dates(table='mitma_trips_matrix', date1=date1, date2=date2, province_groups=province_group)\n",
    "df_flux_daily['datetime'] = pd.to_datetime(df_flux_daily['datetime'], format='%Y-%m-%m')\n",
    "df_flux_daily['source'] = df_flux_daily['source'].astype(str)\n",
    "df_flux_daily['target'] = df_flux_daily['target'].astype(str)\n",
    "\n",
    "# Exclude nodes outside of Catalunya\n",
    "df = pd.DataFrame()\n",
    "for mg in province_group:\n",
    "    postal_code = mg[1][0]\n",
    "    df = df.append(df_flux_daily[(df_flux_daily['source'].str[:2] == postal_code)])\n",
    "df_flux_daily = df\n",
    "\n",
    "source_nodes = list(df_flux_daily['source'].unique())\n",
    "df_flux_daily = df_flux_daily[df_flux_daily['target'].isin(source_nodes)]\n",
    "\n",
    "# Remove the dot that indicates thousands in the trips column\n",
    "df_flux_daily['trips'] = df_flux_daily['trips'].apply(round)\n",
    "\n",
    "# Describe dataframe\n",
    "print(f\"df_flux_daily size: {df_flux_daily.size}\")\n",
    "df_flux_daily.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create nodes dataframe (source and source_labels)\n",
    "all_nodes = list(df_flux_daily['source'].unique())\n",
    "df_nodes = pd.DataFrame(all_nodes)\n",
    "df_nodes.reset_index(inplace=True)\n",
    "df_nodes.columns = ['source', 'source_label']\n",
    "\n",
    "# Add source nodes to flux dataframe\n",
    "df_flux_daily = pd.merge(\n",
    "    df_nodes, df_flux_daily, how=\"inner\",\n",
    "    left_on='source_label', right_on='source', suffixes=(\"\", \"_y\"))\n",
    "df_flux_daily.drop(['source_y'], axis=1, inplace=True)\n",
    "\n",
    "# Add target nodes to flux dataframe\n",
    "df_nodes = df_nodes.rename(columns={'source': 'target', 'source_label': 'target_label'})\n",
    "df_flux_daily = pd.merge(\n",
    "    df_nodes, df_flux_daily, how=\"inner\", \n",
    "    left_on='target_label', right_on='target', suffixes=(\"\", \"_y\"))\n",
    "df_flux_daily.drop(['target_y'], axis=1, inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "df_flux_daily = df_flux_daily[['datetime', 'source', 'source_label', 'target', 'target_label', 'trips']]\n",
    "df_flux_daily = df_flux_daily.sort_values(['source', 'datetime', 'target'])\n",
    "df_flux_daily.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Low trips edges removal <a class=\"anchor\" id=\"removal\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study trips distribution\n",
    "df_flux_daily['trips'].hist(bins=100)\n",
    "plt.show()\n",
    "pd.DataFrame(df_flux_daily['trips'].quantile([0.05, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 1])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 5% of edges with lower number of trips\n",
    "df_flux_daily = df_flux_daily[df_flux_daily['trips'] >= 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Region geometries, centroids and geographical distances  <a class=\"anchor\" id=\"geography-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get geometry info from qrp indexes\n",
    "df_qrp = query_trips_iio_or_qrp_between_dates(table='mitma_qrp', province_groups=province_group)\n",
    "df_geometry = df_qrp[['source', 'geometry']].reset_index(drop=True).drop_duplicates()\n",
    "\n",
    "# Get centroids\n",
    "df_geometry['centroid'] = df_geometry['geometry'].centroid\n",
    "\n",
    "# Add node's names and labels to geometry dataframe\n",
    "df_geometry.columns = ['source_label', 'geometry', 'centroid']\n",
    "df_nodes.columns = ['source', 'source_label']\n",
    "\n",
    "df_geometry = pd.merge(\n",
    "    df_nodes, df_geometry, how=\"inner\",\n",
    "    left_on='source_label', right_on='source_label')\n",
    "df_geometry.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute geographical distances between two centroids\n",
    "a = gpd.GeoSeries(df_geometry['centroid'], crs=\"EPSG:4326\")\n",
    "b = a.copy()\n",
    "centroid_distances = a.apply(lambda g: b.distance(g))\n",
    "\n",
    "# Matrix to dataframe \n",
    "df_distances = pd.melt(centroid_distances.reset_index(), id_vars=['index'], var_name='description')\n",
    "df_distances.columns = ['source', 'target', 'geo_distance']\n",
    "df_distances['geo_distance'] = round(df_distances['geo_distance']/1000, 3)\n",
    "\n",
    "# Distance binning\n",
    "labels = ['<10', '10-50', '50-200', '200-600']\n",
    "bins = [0, 10, 50, 200, 600]\n",
    "df_distances['geo_distance_b'] = pd.cut(df_distances['geo_distance'], bins=bins, labels=labels, right=True)\n",
    "df_distances.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Graphs creation for each COVID-19 phase <a class=\"anchor\" id=\"graphs\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inverse_trips(df):\n",
    "    \"\"\"\n",
    "    Compute inverse of number of trips\n",
    "    \"\"\"    \n",
    "    # Inverse p\n",
    "    df['inverse_trips'] = 1 / df['trips']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_digraph(df, df_geometry, df_distances):\n",
    "    \"\"\"\n",
    "    Creates directed graph from a dataframe with source, target, trips, inverse_trips and total_trips columns\n",
    "    \"\"\"\n",
    "    # Add nodes\n",
    "    G = nx.from_pandas_edgelist(df, source='source', target='target', \n",
    "        edge_attr=['trips', 'inverse_trips'], create_using=nx.DiGraph)\n",
    "\n",
    "    # Add nodes geometries, centroids and labels\n",
    "    for index, row in df_geometry.iterrows():\n",
    "        G.nodes[row['source']]['geometry'] = row['geometry']\n",
    "        G.nodes[row['source']]['centroid'] = row['centroid']\n",
    "        G.nodes[row['source']]['label'] = row['source_label']\n",
    "\n",
    "    # Add edges geographical distances\n",
    "    for index, row in df_distances.iterrows():\n",
    "        # Graphs don't have edges with same source and target\n",
    "        #if row['source'] ==  row['target']:\n",
    "        #    continue\n",
    "        # Graphs don't have all combinations of source and target\n",
    "        try:\n",
    "            G.edges[row['source'], row['target']]['geo_distance'] = row['geo_distance']\n",
    "            G.edges[row['source'], row['target']]['geo_distance_b'] = row['geo_distance_b']\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_compute_dataframe_and_graph(df, covid_phase, save_url, df_geometry, df_distances, compute=False):\n",
    "    \"\"\"\n",
    "    Loads or computes the dataframes and graphs of the different covid phases. \n",
    "    Also dataframes and graphs are saved if doesn't exist the file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dataframe\n",
    "    if (exists(f'{save_url}/{phases_names[covid_phase]}_df.csv')) & (compute == False):\n",
    "        df = pd.read_csv(f'{save_url}/{phases_names[covid_phase]}_df.csv')\n",
    "    else:\n",
    "        df = compute_inverse_trips(df)\n",
    "        df.to_csv(f'{save_url}/{phases_names[covid_phase]}_df.csv', index=False)\n",
    "    \n",
    "    # Graph\n",
    "    if (exists(f'{save_url}/{phases_names[covid_phase]}_graph.gpickle')) & (compute == False):\n",
    "        G = nx.read_gpickle(f'{save_url}/{phases_names[covid_phase]}_graph.gpickle')\n",
    "    else:\n",
    "        G = create_digraph(df, df_geometry, df_distances)\n",
    "        nx.write_gpickle(G, f'{save_url}/{phases_names[covid_phase]}_graph.gpickle')\n",
    "        \n",
    "    # Describe Graph\n",
    "    print(f'Number of nodes: {G.number_of_nodes()}')\n",
    "    print(f'Number of edges: {G.number_of_edges()}\\n')\n",
    "    \n",
    "    return df, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframes for each covid phases and compute mean p mobility index\n",
    "df_precovid = df_flux_daily[\n",
    "    (df_flux_daily['datetime'] >= phases_list[0]['precovid']['start']) & \n",
    "    (df_flux_daily['datetime'] <= phases_list[0]['precovid']['end'  ])]\n",
    "df_precovid = df_precovid.groupby(['source', 'source_label', 'target', 'target_label']).mean().reset_index()\n",
    "\n",
    "df_lockdown = df_flux_daily[\n",
    "    (df_flux_daily['datetime'] >= phases_list[1]['lockdown']['start']) & \n",
    "    (df_flux_daily['datetime'] <= phases_list[1]['lockdown']['end'  ])]\n",
    "df_lockdown = df_lockdown.groupby(['source', 'source_label', 'target', 'target_label']).mean().reset_index()\n",
    "\n",
    "df_mobilitat_essenc = df_flux_daily[\n",
    "    (df_flux_daily['datetime'] >= phases_list[2]['mobilitat_essenc']['start']) & \n",
    "    (df_flux_daily['datetime'] <= phases_list[2]['mobilitat_essenc']['end'  ])]\n",
    "df_mobilitat_essenc = df_mobilitat_essenc.groupby(['source', 'source_label', 'target', 'target_label']).mean().reset_index()\n",
    "\n",
    "df_fase_0 = df_flux_daily[\n",
    "    (df_flux_daily['datetime'] >= phases_list[3]['fase_0']['start']) & \n",
    "    (df_flux_daily['datetime'] <= phases_list[3]['fase_0']['end'  ])]\n",
    "df_fase_0 = df_fase_0.groupby(['source', 'source_label', 'target', 'target_label']).mean().reset_index()\n",
    "\n",
    "df_desescalada = df_flux_daily[\n",
    "    (df_flux_daily['datetime'] >= phases_list[4]['desescalada']['start']) & \n",
    "    (df_flux_daily['datetime'] <= phases_list[4]['desescalada']['end'  ])]\n",
    "df_desescalada = df_desescalada.groupby(['source', 'source_label', 'target', 'target_label']).mean().reset_index()\n",
    "\n",
    "df_no_restriccions = df_flux_daily[\n",
    "    (df_flux_daily['datetime'] >= phases_list[5]['no_restriccions']['start']) & \n",
    "    (df_flux_daily['datetime'] <= phases_list[5]['no_restriccions']['end'  ])]\n",
    "df_no_restriccions = df_no_restriccions.groupby(['source', 'source_label', 'target', 'target_label']).mean().reset_index()\n",
    "\n",
    "df_alerta_5_inici = df_flux_daily[\n",
    "    (df_flux_daily['datetime'] >= phases_list[6]['alerta_5_inici']['start']) & \n",
    "    (df_flux_daily['datetime'] <= phases_list[6]['alerta_5_inici']['end'  ])]\n",
    "df_alerta_5_inici = df_alerta_5_inici.groupby(['source', 'source_label', 'target', 'target_label']).mean().reset_index()\n",
    "\n",
    "df_alerta_5_tr1 = df_flux_daily[\n",
    "    (df_flux_daily['datetime'] >= phases_list[7]['alerta_5_tr1']['start']) & \n",
    "    (df_flux_daily['datetime'] <= phases_list[7]['alerta_5_tr1']['end'  ])]\n",
    "df_alerta_5_tr1 = df_alerta_5_tr1.groupby(['source', 'source_label', 'target', 'target_label']).mean().reset_index()\n",
    "\n",
    "df_alerta_5_tr1_2 = df_flux_daily[\n",
    "    (df_flux_daily['datetime'] >= phases_list[8]['alerta_5_tr1_2']['start']) & \n",
    "    (df_flux_daily['datetime'] <= phases_list[8]['alerta_5_tr1_2']['end'  ])]\n",
    "df_alerta_5_tr1_2 = df_alerta_5_tr1_2.groupby(['source', 'source_label', 'target', 'target_label']).mean().reset_index()\n",
    "\n",
    "df_nadal_cap_reis = df_flux_daily[\n",
    "    (df_flux_daily['datetime'] >= phases_list[9]['nadal']['start']) & \n",
    "    (df_flux_daily['datetime'] <= phases_list[9]['nadal']['end'  ])]\n",
    "df_nadal_cap_reis = df_nadal_cap_reis.groupby(['source', 'source_label', 'target', 'target_label']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or compute dataframes and graphs for each phase\n",
    "print(\"Precovid\")\n",
    "df_precovid_mean, G_precovid = load_or_compute_dataframe_and_graph(\n",
    "    df_precovid, 0, save_url, df_geometry, df_distances)\n",
    "\n",
    "print(\"Lockdown\")\n",
    "df_lockdown_mean, G_lockdown = load_or_compute_dataframe_and_graph(\n",
    "    df_lockdown, 1, save_url, df_geometry, df_distances)\n",
    "\n",
    "print(\"Mobilitat essencial\")\n",
    "df_mobilitat_mean, G_mobilitat = load_or_compute_dataframe_and_graph(\n",
    "    df_mobilitat_essenc, 2, save_url, df_geometry, df_distances)\n",
    "\n",
    "print(\"Fase 0\")\n",
    "df_fase0_mean, G_fase0 = load_or_compute_dataframe_and_graph(\n",
    "    df_fase_0, 3, save_url, df_geometry, df_distances)\n",
    "\n",
    "print(\"Desescalada\")\n",
    "df_desescalada_mean, G_desescalada = load_or_compute_dataframe_and_graph(\n",
    "    df_desescalada, 4, save_url, df_geometry, df_distances)\n",
    "\n",
    "print(\"No restriccions\")\n",
    "df_no_restriccions_mean, G_no_restriccions = load_or_compute_dataframe_and_graph(\n",
    "    df_no_restriccions, 5, save_url, df_geometry, df_distances)\n",
    "\n",
    "print(\"Alerta 5 inici\")\n",
    "df_alert5_mean, G_alert5 = load_or_compute_dataframe_and_graph(\n",
    "    df_alerta_5_inici, 6, save_url, df_geometry, df_distances)\n",
    "\n",
    "print(\"Alerta 5 inici tram 1\")\n",
    "df_alert5_tr1_mean, G_alert5_tr1 = load_or_compute_dataframe_and_graph(\n",
    "    df_alerta_5_tr1, 7, save_url, df_geometry, df_distances)\n",
    "\n",
    "print(\"Alerta 5 inici tram 1__2\")\n",
    "df_alert5_tr2_mean, G_alert5_tr2 = load_or_compute_dataframe_and_graph(\n",
    "    df_alerta_5_tr1_2, 8, save_url, df_geometry, df_distances)\n",
    "\n",
    "print(\"Nadal, cap d'any, reis mags\")\n",
    "df_xmas, G_xmas = load_or_compute_dataframe_and_graph(\n",
    "    df_nadal_cap_reis, 9, save_url, df_geometry, df_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Graphs analysis <a class=\"anchor\" id=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare lists with study df, graphs and titles\n",
    "list_titles = [\n",
    "    'Precovid', 'Lockdown', 'Mobilitat essencial', \n",
    "    'Fase 0', 'Desescalada', 'No restriccions', \n",
    "    'Alerta 5 inici', 'Alerta 5 tram 1', 'Alerta 5 flexible', \n",
    "    'Nadal, cap d\\'any, reis mags']\n",
    "\n",
    "list_df = [\n",
    "    df_precovid_mean, df_lockdown_mean, df_mobilitat_mean, \n",
    "    df_fase0_mean, df_desescalada_mean, df_no_restriccions_mean,\n",
    "    df_alerta_5_inici, df_alerta_5_tr1, df_alerta_5_tr1_2, \n",
    "    df_nadal_cap_reis]\n",
    "\n",
    "list_G = [\n",
    "    G_precovid, G_lockdown, G_mobilitat, G_fase0, \n",
    "    G_desescalada, G_no_restriccions, G_alert5, G_alert5_tr1, \n",
    "    G_alert5_tr2, G_xmas]\n",
    "\n",
    "print(len(list_titles), len(list_df), len(list_G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter graph by geographical distance edge attribute\n",
    "# Get edges with each distance label \n",
    "edges_by_distance = {}\n",
    "for label in labels:\n",
    "    nodes = []\n",
    "    for source, target, edge_attr in G_precovid.edges(data=True):\n",
    "        if edge_attr['geo_distance_b'] == label:\n",
    "            nodes.append((source, target))\n",
    "    edges_by_distance[label] = nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_graphs_by_distance(G, edges_by_distance):\n",
    "    \"\"\"\n",
    "    Creates 4 graphs grouping the nodes which have a certain edge attribute \n",
    "    distance value. \n",
    "    \"\"\"\n",
    "    G_10  = G.edge_subgraph(edges_by_distance['<10'])\n",
    "    G_50  = G.edge_subgraph(edges_by_distance['10-50'])\n",
    "    G_200 = G.edge_subgraph(edges_by_distance['50-200'])\n",
    "    G_600 = G_precovid.edge_subgraph(edges_by_distance['200-600'])\n",
    "    \n",
    "    return G_10, G_50, G_200, G_600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Mobility map <a class=\"anchor\" id=\"mobility-map\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df_flux_daily['trips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph_map_mobility(G, df_geometry, title):\n",
    "    \n",
    "    # Get edge normalized weights\n",
    "    nodes = G.nodes\n",
    "    edges = [G[u][v] for u,v in G.edges]\n",
    "    weights = [i['trips']/324985 for i in edges]\n",
    "\n",
    "    # Get node positions\n",
    "    gpd_geometry = gpd.GeoDataFrame(df_geometry, crs=\"EPSG:4326\", geometry='geometry')\n",
    "    gpd_geometry[\"centroid\"] = gpd_geometry[\"geometry\"].centroid\n",
    "    gpd_geometry[\"x\"] = gpd_geometry.centroid.map(lambda p: p.x)\n",
    "    gpd_geometry[\"y\"] = gpd_geometry.centroid.map(lambda p: p.y)\n",
    "    centroids = np.column_stack((gpd_geometry['x'], gpd_geometry['y']))\n",
    "    positions = dict(zip(nodes, centroids))\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    gpd_geometry['geometry'].plot(ax=ax, edgecolor='white', alpha=0.4)\n",
    "    nx.draw(\n",
    "        G.subgraph(nodes), pos=positions,\n",
    "        node_size=4, width=weights, arrows=False,         \n",
    "        edge_vmin=0, edge_vmax=1)\n",
    "    \n",
    "    fig.suptitle(f'{title}', size=16, y=0.85)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Sum of mean trips in {title}: {round(np.sum([i['trips'] for i in edges]))},  \" + \n",
    "          f\"mean: {round(np.mean([i['trips'] for i in edges]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for G, title in zip(list_G, list_titles):\n",
    "    plot_graph_map_mobility(G, df_geometry, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Degree centrality and density <a class=\"anchor\" id=\"degree-density\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes with more degrees and distribution\n",
    "degrees = pd.DataFrame(G_precovid.degree(), columns=['node','degree'])\n",
    "degrees['degree'].hist()\n",
    "plt.show()\n",
    "\n",
    "# MITMA regions and number of degrees\n",
    "print(df_nodes[df_nodes['source'].isin([15, 17, 22])])\n",
    "degrees[degrees['node'].isin([15, 17, 22])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_histogram(list_G, list_titles, edges_by_distance, by_distance=False):\n",
    "    \n",
    "    fig, axs = plt.subplots(5,2,  figsize=(14, 9), sharey=True, sharex=True)\n",
    "    \n",
    "    # For each graph\n",
    "    row = 0\n",
    "    column = 0\n",
    "    for i, G in enumerate(list_G):\n",
    "        \n",
    "        not_modified = True\n",
    "        \n",
    "        # Plot degrees\n",
    "        degrees = [G.degree(n) for n in G.nodes()]\n",
    "        pd.DataFrame(degrees).hist(ax=axs[column][row])\n",
    "        \n",
    "        # Compute average degree and density\n",
    "        average = np.mean(degrees)\n",
    "        density = nx.density(G)\n",
    "        axs[column][row].set_title(f\"{list_titles[i]}: {round(density, 3)} density\")\n",
    "        \n",
    "        # Plot average\n",
    "        axs[column][row].axvline(x=average, color='b', label='average degree')\n",
    "        \n",
    "        # Rows and columns update\n",
    "        if row == 0:\n",
    "            row += 1\n",
    "            not_modified = False\n",
    "        \n",
    "        if (row == 1) & not_modified:\n",
    "            column += 1\n",
    "            row = 0\n",
    "        \n",
    "    fig.suptitle(f'Degree frequencies', size=16, y=1)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # For each graph \n",
    "    if by_distance:\n",
    "        fig, axs = plt.subplots(len(list_G), 4, figsize=(14, 15), sharey=True, sharex=True)\n",
    "        fig.suptitle(f'Degree frequencies', size=16, y=1)\n",
    "        \n",
    "        for i, G in enumerate(list_G):\n",
    "            # Filter nodes by distance\n",
    "            G_10, G_50, G_200, G_600 = filter_graphs_by_distance(G, edges_by_distance)\n",
    "\n",
    "            pd.DataFrame([G_10.out_degree(n) for n in G_10.nodes()]).hist(ax=axs[i][0], bins=10)\n",
    "            pd.DataFrame([G_50.out_degree(n) for n in G_50.nodes()]).hist(ax=axs[i][1], bins=10)\n",
    "            pd.DataFrame([G_200.out_degree(n) for n in G_200.nodes()]).hist(ax=axs[i][2], bins=10)\n",
    "            pd.DataFrame([G_600.out_degree(n) for n in G_600.nodes()]).hist(ax=axs[i][3], bins=10)\n",
    "            \n",
    "            for col in range(4):\n",
    "                axs[i][col].set_title('')\n",
    "            axs[i][0].set_ylabel(f'{list_titles[i]}')\n",
    "\n",
    "        # Set title\n",
    "        axs[0][0].set_title(f'{labels[0]} Km')\n",
    "        axs[0][1].set_title(f'{labels[1]} Km')\n",
    "        axs[0][2].set_title(f'{labels[2]} Km')\n",
    "        axs[0][3].set_title(f'{labels[3]} Km')\n",
    "            \n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_degree_histogram(list_G, list_titles, edges_by_distance, by_distance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. In-degree and Out-degree centralities <a class=\"anchor\" id=\"in-out\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In/Out-degree of a node\n",
    "target_mitma = '0801903'\n",
    "SOURCE = df_nodes[df_nodes['source_label'] == target_mitma]['source'].values[0]\n",
    "\n",
    "for metric in metrics:\n",
    "    print(\"\\n\", metric)\n",
    "    for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "        in_degree = G.in_degree(nbunch=[SOURCE])\n",
    "        out_degree = G.out_degree(nbunch=[SOURCE])\n",
    "        print(\"\\t\", title,\":\", in_degree[0],\"/\", out_degree[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_in_degree_histogram(list_G, list_titles, by_distance=False):\n",
    "    \n",
    "    fig, axs = plt.subplots(len(list_G),2,  figsize=(2*len(list_G),3*len(list_G)))\n",
    "    \n",
    "    # For each graph\n",
    "    for i, G in enumerate(list_G):\n",
    "        \n",
    "        # Plot in_degrees\n",
    "        in_degrees = [G.in_degree(n) for n in G.nodes()]\n",
    "        pd.DataFrame(in_degrees).hist(ax=axs[i][0])\n",
    "        average = np.mean(in_degrees)\n",
    "        density = nx.density(G)\n",
    "        axs[i][0].set_title(f\"{list_titles[i]}: {round(density, 3)} density\")\n",
    "        \n",
    "        # Plot average\n",
    "        axs[i][0].axvline(x=average, color='b', label='average degree')\n",
    "        \n",
    "        # Plot out_degrees\n",
    "        out_degrees = [G.out_degree(n) for n in G.nodes()]\n",
    "        pd.DataFrame(out_degrees).hist(ax=axs[i][1])\n",
    "        average = np.mean(out_degrees)\n",
    "        density = nx.density(G)\n",
    "        axs[i][1].set_title(f\"{list_titles[i]}\")\n",
    "        \n",
    "        # Plot average\n",
    "        axs[i][1].axvline(x=average, color='b', label='average degree')\n",
    "        \n",
    "    fig.suptitle(f'In-Degree and Out-Degree frequencies', size=16, y=1)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_in_degree_histogram(list_G[:5], list_titles[:5], by_distance=False)\n",
    "plot_in_degree_histogram(list_G[5:], list_titles[5:], by_distance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Distance probability <a class=\"anchor\" id=\"distance-prob\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distance_probability(list_G, list_titles):\n",
    "    fig, axs = plt.subplots(5,2, figsize=(14,9), sharey=True, sharex=True)\n",
    "    \n",
    "    column = 0\n",
    "    row = 0\n",
    "    \n",
    "    # For each graph\n",
    "    for i, G in enumerate(list_G):\n",
    "        # Row and columns update\n",
    "        not_modified = True\n",
    "        \n",
    "        edges = [G[u][v] for u,v in G.edges]\n",
    "        weights = [i['geo_distance'] for i in edges]\n",
    "        axs[column][row].hist(weights, bins=100, histtype='step', density=True, cumulative=-1)\n",
    "\n",
    "        axs[column][row].set_title(f\"{list_titles[i]}\")\n",
    "        axs[4][row].set_xlabel(\"Km\")\n",
    "        axs[column][0].set_ylabel(\"P(d_ij >= d)\")\n",
    "        \n",
    "        axs[column][row].set_yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "        axs[column][row].grid()\n",
    "        \n",
    "        # Rows and columns update\n",
    "        if row == 0:\n",
    "            row += 1\n",
    "            not_modified = False\n",
    "        if (row == 1) & not_modified:\n",
    "            column += 1\n",
    "            row = 0\n",
    "        \n",
    "\n",
    "    fig.suptitle(f'Distance probability', size=16, y=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distance_probability(list_G, list_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Shortest Path and their Lengths $L_{ij}$ <a class=\"anchor\" id=\"shortest-paths\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shortest_paths(G, metric, weighted=True, compute_paths=False):\n",
    "    \"\"\"\n",
    "    Computes shortest paths of a Graph Network, their lengths, and the average\n",
    "    shortest path length of the network.\n",
    "    \"\"\"\n",
    "    if weighted:\n",
    "        if compute_paths:\n",
    "            # Compute shortest path and their lenghts\n",
    "            shortest_paths = nx.shortest_path(G, method='dijkstra', weight=metric)\n",
    "            shortest_path_lengths = list(nx.shortest_path_length(G, method='dijkstra', weight=metric))\n",
    "        else:\n",
    "            shortest_paths, shortest_path_lengths = None, None\n",
    "\n",
    "        # Average shortest path length\n",
    "        average_shortest_path_length = nx.average_shortest_path_length(G, method='dijkstra', weight=metric)\n",
    "    \n",
    "    else:\n",
    "        if compute_paths:\n",
    "            # Compute shortest path and their lenghts\n",
    "            shortest_paths = nx.shortest_path(G)\n",
    "            shortest_path_lengths = nx.shortest_path_length(G)\n",
    "        else:\n",
    "            shortest_paths, shortest_path_lengths = None, None\n",
    "\n",
    "        # Average shortest path length\n",
    "        average_shortest_path_length = nx.average_shortest_path_length(G)\n",
    "        \n",
    "    return average_shortest_path_length, shortest_paths, shortest_path_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_and_lengths_to_dataframe(shortest_paths, shortest_path_lengths):\n",
    "    \"\"\"\n",
    "    Creates a dataframe from merging shortest_paths and the shortest_path_lengths \n",
    "    originated from a Graph using the library Networkx.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Shortest paths\n",
    "    df_shortest_paths = pd.DataFrame()\n",
    "    for source, target_path in enumerate(shortest_paths.items()):\n",
    "        for target, path in target_path[1].items():\n",
    "            df_shortest_paths = df_shortest_paths.append([[source, target, path]])\n",
    "    df_shortest_paths.columns = ['source', 'target', 'path']\n",
    "\n",
    "    # Shortest path lengths\n",
    "    df_shortest_path_lengths = pd.DataFrame()\n",
    "    for source, target_distance_dict in shortest_path_lengths:\n",
    "        for target, distance in target_distance_dict.items():\n",
    "            df_shortest_path_lengths = df_shortest_path_lengths.append([[source, target, distance]])\n",
    "    df_shortest_path_lengths.columns = ['source', 'target', 'distance']\n",
    "    \n",
    "    # Merge dataframes\n",
    "    df_shortest_paths_and_path_lengths = pd.merge(df_shortest_paths, df_shortest_path_lengths, how=\"inner\")\n",
    "    \n",
    "    return df_shortest_paths_and_path_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute shortests paths and their lengths\n",
    "METRICS = ['inverse_trips', 'geo_distance']\n",
    "\n",
    "for METRIC in METRICS:\n",
    "\n",
    "    # Compute all paths and their lengths\n",
    "    for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "        print(title)\n",
    "\n",
    "        if exists(f'{save_url}/{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{METRIC}.csv'):\n",
    "            continue\n",
    "        else:\n",
    "            # Compute shortest paths and their lengths\n",
    "            average_len, shortest_paths, shortest_path_lengths = compute_shortest_paths(\n",
    "                G, METRIC, weighted=True, compute_paths=True)\n",
    "\n",
    "            # Paths and lengths to csv\n",
    "            df_shortest_paths_and_path_lengths = path_and_lengths_to_dataframe(\n",
    "                shortest_paths, shortest_path_lengths)\n",
    "            df_shortest_paths_and_path_lengths.to_csv(\n",
    "                f'{save_url}{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{METRIC}.csv',\n",
    "                index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shortest_paths(df, title, metric):\n",
    "    \n",
    "    fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "    \n",
    "    # Single variables\n",
    "    df['len'].hist(ax=axs[0])\n",
    "    df['distance'].hist(ax=axs[1])\n",
    "    \n",
    "    # Plot vertical lines\n",
    "    axs[0].axvline(x=df['len'].mean(), color='b')\n",
    "    axs[1].axvline(x=df['distance'].mean(), color='b')\n",
    "    \n",
    "    # 2 variables linear regression and R^2\n",
    "    x = df['len'].values.reshape(-1, 1)\n",
    "    y = df['distance'].values.reshape(-1, 1)\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x, y)\n",
    "    plt.plot(x, regr.predict(x), color='red', linewidth=3)\n",
    "    r2 = round(regr.score(y, x), 2)\n",
    "    m = round(regr.coef_[0][0], 2)\n",
    "    \n",
    "    # 2 variables scatter plot\n",
    "    df.plot.scatter(x='len', y='distance', ax=axs[2])\n",
    "    \n",
    "    if metric == 'inverse_trips':\n",
    "        # Axis limits\n",
    "        axs[0].set_xlim([0, 3.5])\n",
    "        axs[0].set_ylim([0, 180_000])\n",
    "\n",
    "        axs[1].set_xlim([0, 2.5])\n",
    "        axs[1].set_ylim([0, 180_000])\n",
    "\n",
    "        axs[2].set_xlim([0, 2.5])\n",
    "        axs[2].set_ylim([0, 3.5])\n",
    "    \n",
    "    elif metric == 'geo_distance':\n",
    "        # Axis limits\n",
    "        axs[0].set_xlim([0, 8])\n",
    "        axs[0].set_ylim([0, 175_000])\n",
    "\n",
    "        axs[1].set_xlim([0, 310])\n",
    "        axs[1].set_ylim([0, 50_000])\n",
    "\n",
    "        axs[2].set_xlim([0, 10])\n",
    "        axs[2].set_ylim([0, 320])\n",
    "\n",
    "    # Set titles\n",
    "    axs[0].set_title('Shortest path')\n",
    "    axs[1].set_title('Shortest path length')\n",
    "    axs[2].set_title(f'Length vs Distance (m: {m}, R2: {r2})')\n",
    "    \n",
    "    # x labels\n",
    "    axs[0].set_xlabel('Number of nodes from source to target')\n",
    "    \n",
    "    if metric == 'inverse_trips':\n",
    "        axs[1].set_xlabel('Sum of 1/trips from source to target')\n",
    "    elif metric == 'geo_distance':\n",
    "        axs[1].set_xlabel('Sum of km from source to target')\n",
    "    \n",
    "    # Figure title\n",
    "    fig.suptitle(title, size=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot shortest paths and lengths' statistics\n",
    "metrics = ['geo_distance', 'inverse_trips']\n",
    "\n",
    "for metric in metrics:\n",
    "    for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                f'{save_url}/{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{metric}.csv', \n",
    "                index_col='Unnamed: 0')\n",
    "        except:\n",
    "            df = pd.read_csv(\n",
    "                f'{save_url}/{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{metric}.csv')\n",
    "\n",
    "        # Format path column to list\n",
    "        df['path'] = df['path'].str.replace(\"[\", \"\")\n",
    "        df['path'] = df['path'].str.replace(\"]\", \"\")\n",
    "        df['path'] = df['path'].str.split(\", \")\n",
    "\n",
    "        # Compute length of the path\n",
    "        df['len'] = df['path'].apply(len)\n",
    "        title = f'{title} {metric}: Average Shortest Path {round(df[\"len\"].mean(), 2)}, Length: {round(df[\"distance\"].mean(), 2)}'\n",
    "\n",
    "        plot_shortest_paths(df, title, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitter import Fitter, get_common_distributions, get_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length and distance distributions across phases\n",
    "for metric in ['geo_distance', 'inverse_trips']:\n",
    "    # Create LENGTH's figures\n",
    "    fig, axs = plt.subplots(1, 10, figsize=(14, 2.5), sharey=True)\n",
    "    fig.suptitle(f\"Shortest path's lengths distributions\", size=16)\n",
    "    \n",
    "    for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "        # Open file\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                f'{save_url}/{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{metric}.csv', \n",
    "                index_col='Unnamed: 0')\n",
    "        except:\n",
    "            df = pd.read_csv(\n",
    "                f'{save_url}/{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{metric}.csv')\n",
    "\n",
    "        # Create len column\n",
    "        df['path'] = df['path'].str.replace(\"[\", \"\")\n",
    "        df['path'] = df['path'].str.replace(\"]\", \"\")\n",
    "        df['path'] = df['path'].str.split(\", \")\n",
    "        df['len'] = df['path'].apply(len)\n",
    "\n",
    "        # Plot\n",
    "        df['len'].hist(ax=axs[covid_phase], bins=8)\n",
    "        axs[covid_phase].axvline(x=df['len'].mean(), color='b')\n",
    "        axs[covid_phase].annotate(\n",
    "            xycoords='axes fraction', xy=(0.35, 0.9), text=f\"mean: {str(round(df['len'].mean(), 2))}\", size=8)\n",
    "        \n",
    "        # Title and axis\n",
    "        axs[covid_phase].set_title(title, size=10)\n",
    "        \n",
    "        if metric == 'geo_distance':\n",
    "            axs[covid_phase].set_xticks([0, 2, 4, 6, 8])\n",
    "        elif (metric == 'inverse_trips') & :\n",
    "            axs[covid_phase].set_xticks([0, 5, 10, 15, 20, 25])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create DISTANCE's figures\n",
    "    fig, axs = plt.subplots(1, 10, figsize=(16, 2.5), sharey=True)\n",
    "    fig.suptitle(f\"Shortest paths' distances\", size=16)\n",
    "    \n",
    "    for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "        # Open file\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                f'{save_url}/{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{metric}.csv', \n",
    "                index_col='Unnamed: 0')\n",
    "        except:\n",
    "            df = pd.read_csv(\n",
    "                f'{save_url}/{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{metric}.csv')\n",
    "\n",
    "        # Plot\n",
    "        df['distance'].hist(ax=axs[covid_phase])\n",
    "        axs[covid_phase].set_title(title, size=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create DISTANCE's filtered distributions figures\n",
    "for metric in ['geo_distance', 'inverse_trips']:\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(5, 10, figsize=(16, 11), sharey='row', sharex=True)\n",
    "    fig.suptitle(f\"Distance distribution by shortest path length value\", size=16)\n",
    "    \n",
    "    for i in range(1, 5+1):\n",
    "        \n",
    "        for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "            # Open file\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    f'{save_url}/{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{metric}.csv', \n",
    "                    index_col='Unnamed: 0')\n",
    "            except:\n",
    "                df = pd.read_csv(\n",
    "                    f'{save_url}/{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{metric}.csv')\n",
    "\n",
    "            # Create len column\n",
    "            df['path'] = df['path'].str.replace(\"[\", \"\")\n",
    "            df['path'] = df['path'].str.replace(\"]\", \"\")\n",
    "            df['path'] = df['path'].str.split(\", \")\n",
    "            df['len'] = df['path'].apply(len)\n",
    "            \n",
    "            # Filter distance by len\n",
    "            if i == 5:\n",
    "                df_filtered = df[df['len'] > 5]\n",
    "            elif i > 5:\n",
    "                continue\n",
    "            else:\n",
    "                df_filtered = df[df['len'] == i]\n",
    "            \n",
    "            # Plot subfigure\n",
    "            df_filtered['distance'].hist(ax=axs[i-1][covid_phase])\n",
    "            axs[0][covid_phase].set_title(title, size=10)\n",
    "            axs[4][covid_phase].set_xlabel('Km')\n",
    "            axs[i-1][0].set_ylabel(f\"Len {i}\")\n",
    "            axs[4][covid_phase].set_xticks([0, 100, 200])\n",
    "            \n",
    "            if i == 5:\n",
    "                axs[i-1][0].set_ylabel(f\"Len >={i}\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test distance distribution in each phase grouping by len of the path\n",
    "for metric in ['geo_distance']:\n",
    "    for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "        print(title)\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                f'{save_url}/{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{metric}.csv', \n",
    "                index_col='Unnamed: 0')\n",
    "        except:\n",
    "            df = pd.read_csv(\n",
    "                f'{save_url}/{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{metric}.csv')\n",
    "\n",
    "        # Format path column to list\n",
    "        df['path'] = df['path'].str.replace(\"[\", \"\")\n",
    "        df['path'] = df['path'].str.replace(\"]\", \"\")\n",
    "        df['path'] = df['path'].str.split(\", \")\n",
    "        \n",
    "        # Compute length of the path\n",
    "        df['len'] = df['path'].apply(len)\n",
    "        \n",
    "        for i in range(1, max(df['len'])+1):\n",
    "            # Filter distance by len path\n",
    "            if i == 5:\n",
    "                df_filtered = df[df['len'] > 5]\n",
    "            elif i > 5:\n",
    "                continue\n",
    "            else:\n",
    "                df_filtered = df[df['len'] == i]\n",
    "\n",
    "            print(f\"Len {i}\")\n",
    "\n",
    "            distances =  df_filtered['distance'].values\n",
    "            distributions_to_check = [\n",
    "                'chi2', 'expon', 'exponpow', 'gamma', 'lognorm', 'norm', 'powerlaw']\n",
    "            f = Fitter(distances, distributions=distributions_to_check)\n",
    "            f.fit()\n",
    "            print(f.summary()[:2][['sumsquare_error', 'ks_pvalue']])\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "#             statistic, p_value = shapiro(distances)\n",
    "#             if p_value > 0.05:\n",
    "#                 print(\"NORMAL:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Shortest path trees <a class=\"anchor\" id=\"shortest-paths-trees\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_nodes_in_path_file(SOURCE, covid_phase, metric='geo_distance'):\n",
    "\n",
    "    # Load shortest paths csv\n",
    "    try:\n",
    "        df_geo_dist = pd.read_csv(\n",
    "            f'{save_url}/{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{metric}.csv', \n",
    "            index_col='Unnamed: 0')\n",
    "    except:\n",
    "        df_geo_dist = pd.read_csv(f'{save_url}/{phases_names[covid_phase]}_df_shortest_paths_and_lengths_{metric}.csv')\n",
    "\n",
    "    # Filter source node\n",
    "    df = df_geo_dist[(df_geo_dist['source'] == SOURCE) & (df_geo_dist['target'] != SOURCE)]\n",
    "    df = df[['source', 'target', 'distance', 'path']]\n",
    "\n",
    "    # Compute max path length\n",
    "    df['path_list'] = df['path']\n",
    "    df.loc[:, 'path_list'] = df['path_list'].str.replace(f\"\\[\", \"\")\n",
    "    df.loc[:, 'path_list'] = df['path_list'].str.replace(\"]\", \"\")\n",
    "    df.loc[:, 'path_list'] = df['path_list'].str.split(\", \")\n",
    "    max_len = max(df['path_list'].apply(len))\n",
    "\n",
    "    # Extract nodes from list of nodes in paths\n",
    "    df['path_str'] = df['path'].str.replace(f\"\\[\", \"\")\n",
    "    df['path_str'] = df['path_str'].str.replace(\"\\]\", \"\")\n",
    "    children_nodes = df['path_str'].str.split(', ', expand=True)\n",
    "\n",
    "    # Create columns names\n",
    "    new_cols = [f'children_{i+1}' for i in range(0,max_len)]\n",
    "    all_cols = list(df.columns) + new_cols\n",
    "\n",
    "    # Add children nodes to dataframe\n",
    "    df_splitted_paths = pd.concat([df, children_nodes], axis=1)\n",
    "    df_splitted_paths.columns = all_cols\n",
    "\n",
    "    for col in new_cols:\n",
    "        df_splitted_paths[col] = df_splitted_paths[col].apply(str)\n",
    "\n",
    "    return df_splitted_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_digraph_with_one_source_node_using_paths_dataframe(\n",
    "    SOURCE, metric, df_splitted_paths, df_distances, df_covid_phase):\n",
    "    \n",
    "    # Create Digraph with source node\n",
    "    G = nx.DiGraph()\n",
    "    G.add_node(SOURCE)\n",
    "    \n",
    "    # Hierarchy of nodes\n",
    "    children_cols = df_splitted_paths.columns[6:]\n",
    "    \n",
    "    for i, col in enumerate(children_cols):\n",
    "        # First nodes after source node\n",
    "        if col == 'children_1':\n",
    "            for index, row in df_splitted_paths.iterrows():\n",
    "                G.add_edge(SOURCE, int(row[col]))\n",
    "\n",
    "        # Nodes children and their successors\n",
    "        else:\n",
    "            for index, row in df_splitted_paths.iterrows():\n",
    "                if (row[children_cols[i-1]] == 'None') or (row[children_cols[i]] == 'None'):\n",
    "                    continue\n",
    "                else:\n",
    "                    G.add_edge(int(row[children_cols[i-1]]), int(row[children_cols[i]]))    \n",
    "    \n",
    "    if metric == 'geo_distance':\n",
    "        # Add distance between nodes which don't have any node between them to the Graph\n",
    "        for index, row in df_splitted_paths.iterrows():\n",
    "            if len(df_splitted_paths.loc[index,'path_list']) == 1:\n",
    "                G.edges[(row['source'], row['target'])]['len'] = row['distance']\n",
    "\n",
    "        # Add edges geographical distances\n",
    "        for index, row in df_distances.iterrows():\n",
    "            # Graphs don't have all combinations of source and target\n",
    "            try:\n",
    "                G.edges[row['source'], row['target']]['len'] = row[metric]\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    elif metric == 'inverse_trips':\n",
    "        for index, row in df_covid_phase.iterrows():\n",
    "            # Graphs don't have all combinations of source and target\n",
    "            try:\n",
    "                G.edges[row['source'], row['target']]['len'] = row[metric]\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shortest_path_tree(\n",
    "    SOURCE, metric, covid_phase, df_covid_phase, df_distances, df_nodes, \n",
    "    title, place, want_labels=False):\n",
    "    \n",
    "    # From a SOURCE node, split shortest_path nodes in different columns and create digraph \n",
    "    df_splitted_paths = split_nodes_in_path_file(SOURCE, covid_phase=covid_phase, metric=metric)\n",
    "    G = create_digraph_with_one_source_node_using_paths_dataframe(\n",
    "        SOURCE, metric, df_splitted_paths, df_distances, df_covid_phase)\n",
    "    \n",
    "    # Add a context edge to allow comparisons between plots\n",
    "    if metric == 'geo_distance':\n",
    "        G.add_edge(500, 501, len= 300)\n",
    "#     elif metric == 'inverse_trips':\n",
    "#         G.add_edge(500, 501, len= 0.01)\n",
    "\n",
    "    # Figure \n",
    "    fig = plt.figure(dpi=200)\n",
    "    fig.suptitle(f'{title} in {place}: {metric}')\n",
    "    \n",
    "    # Layout\n",
    "    pos = graphviz_layout(G, prog=\"neato\", root=SOURCE)\n",
    "    \n",
    "    # Plot graph\n",
    "    if want_labels:\n",
    "        labels = df_nodes.set_index('source')['source_label']\n",
    "        nx.draw(\n",
    "            G, pos, ax=fig.add_subplot(111), node_size=3, width=0.1, \n",
    "            arrows=True, arrowsize=3, labels=labels, font_size=3, font_color='purple')\n",
    "    else:\n",
    "        nx.draw(G, pos, ax=fig.add_subplot(111), node_size=3, width=0.1, arrows=False, arrowsize=3)\n",
    "    \n",
    "    # Plot Source node in red\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[SOURCE], node_size=4, node_color='red')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return df_splitted_paths, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Sants Estació and Plaça Catalunya's MITMA districts\n",
    "bcn_pc = []\n",
    "for target in df_nodes['source_label'].values:\n",
    "    if '08019' in target:\n",
    "        bcn_pc.append(target)\n",
    "        \n",
    "for target in bcn_pc:\n",
    "    SOURCE = df_nodes[df_nodes['source_label'] == target]['source'].values[0]\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    fig.suptitle(f\"{target}, {SOURCE}\")\n",
    "    a = df_geometry[df_geometry['source_label'] == target]\n",
    "    b = df_geometry[(df_geometry['source_label'].isin(bcn_pc)) & (df_geometry['source_label'] != target)]\n",
    "    gpd_geometry_b = gpd.GeoDataFrame(b, crs=\"EPSG:4326\", geometry='geometry')\n",
    "    gpd_geometry_a = gpd.GeoDataFrame(a, crs=\"EPSG:4326\", geometry='geometry')\n",
    "    gpd_geometry_a['geometry'].plot(color='red', ax=ax)\n",
    "    gpd_geometry_b['geometry'].plot(color='blue', ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sants Estació tree\n",
    "target_mitma = '0801903'\n",
    "SOURCE = df_nodes[df_nodes['source_label'] == target_mitma]['source'].values[0]\n",
    "metrics = ['geo_distance', 'inverse_trips']\n",
    "\n",
    "for metric in metrics:\n",
    "    for covid_phase, (title, df_covid_phase) in enumerate(zip(list_titles, list_df)):\n",
    "        df, G = plot_shortest_path_tree(\n",
    "            SOURCE, metric, covid_phase, df_covid_phase, df_distances, df_nodes, \n",
    "            title, 'Sants Estació', want_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plaça Catalunya tree\n",
    "target_pc = '0801901'\n",
    "SOURCE = df_nodes[df_nodes['source_label'] == target_pc]['source'].values[0]\n",
    "metrics = ['geo_distance', 'inverse_trips']\n",
    "\n",
    "for metric in metrics:\n",
    "    for covid_phase, (title, df_covid_phase) in enumerate(zip(list_titles, list_df)):\n",
    "        df, G = plot_shortest_path_tree(\n",
    "            SOURCE, metric, covid_phase, df_covid_phase, df_distances, df_nodes, \n",
    "            title, 'Plaça Catalunya', want_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Closeness centraliy <a class=\"anchor\" id=\"closeness\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closeness of a node\n",
    "target_mitma = '0801903'\n",
    "SOURCE = df_nodes[df_nodes['source_label'] == target_mitma]['source'].values[0]\n",
    "\n",
    "for metric in metrics:\n",
    "    print(\"\\n\", metric)\n",
    "    for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "        closeness = round(nx.closeness_centrality(G, u=SOURCE, distance=metric, wf_improved=True), 8)\n",
    "        print(\"\\t\", title, closeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Closeness distributions\n",
    "metrics = [None, 'inverse_trips', 'geo_distance']\n",
    "\n",
    "for metric in metrics:\n",
    "   \n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(2,5, figsize=(14, 6), sharey=True, sharex=True)\n",
    "    fig.suptitle(f'Closeness centrality {metric}', size=16)\n",
    "    row, column = 0, 0\n",
    "\n",
    "    for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "\n",
    "        # Row and columns\n",
    "        if covid_phase > 4:\n",
    "            row = 1\n",
    "        if covid_phase == 5:\n",
    "            column = 0\n",
    "        \n",
    "        # Compute closeness\n",
    "        closeness_dict = nx.closeness_centrality(G, distance=metric, wf_improved=True)\n",
    "        df_clos = pd.DataFrame(closeness_dict.values(), columns=['closeness']).reset_index()\n",
    "        \n",
    "        # Plot histogram\n",
    "        df_clos['closeness'].hist(ax=axs[row][column])\n",
    "        \n",
    "        # Plot mean data vertical line\n",
    "        mean = df_clos['closeness'].mean()\n",
    "        axs[row][column].axvline(x=mean, color='b')\n",
    "        \n",
    "        # Title\n",
    "        axs[row][column].set_title(title, size=8)\n",
    "        axs[row][column].annotate(\n",
    "            xycoords='axes fraction', xy=(0.53, 0.9), \n",
    "            text=f\"mean: {str(round(mean, 5))}\", size=7.5)\n",
    "\n",
    "        # Update column\n",
    "        column+=1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['geo_distance']\n",
    "\n",
    "for metric in metrics:\n",
    "   \n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(2,5, figsize=(14, 6), sharey=True, sharex=True)\n",
    "    fig.suptitle(f'Closeness centrality {metric}', size=16)\n",
    "    row, column = 0, 0\n",
    "\n",
    "    for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "\n",
    "        # Row and columns\n",
    "        if covid_phase > 4:\n",
    "            row = 1\n",
    "        if covid_phase == 5:\n",
    "            column = 0\n",
    "        \n",
    "        # Compute closeness\n",
    "        closeness_dict = nx.closeness_centrality(G, distance=metric, wf_improved=True)\n",
    "        df_clos = pd.DataFrame(closeness_dict.values(), columns=['closeness']).reset_index()\n",
    "        \n",
    "        # Plot histogram\n",
    "        df_clos['closeness'].hist(ax=axs[row][column])\n",
    "        \n",
    "        # Plot mean data vertical line\n",
    "        mean = df_clos['closeness'].mean()\n",
    "        axs[row][column].axvline(x=mean, color='b')\n",
    "        \n",
    "        # Title\n",
    "        axs[row][column].set_title(title, size=8)\n",
    "        \n",
    "        if metric != 'geo_distance':\n",
    "            axs[row][column].annotate(\n",
    "                xycoords='axes fraction', xy=(0.53, 0.9), \n",
    "                text=f\"mean: {str(round(mean, 5))}\", size=7.5)\n",
    "        else:\n",
    "            axs[row][column].annotate(\n",
    "                xycoords='axes fraction', xy=(0.05, 0.9), \n",
    "                text=f\"mean: {str(round(mean, 7))}\", size=7.5)\n",
    "\n",
    "        # Update column\n",
    "        column+=1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8. Betweenness centraliy <a class=\"anchor\" id=\"betweenness\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['geo_distance']\n",
    "target_mitma = '0801903'\n",
    "SOURCE = df_nodes[df_nodes['source_label'] == target_mitma]['source'].values[0]\n",
    "\n",
    "for metric in metrics:\n",
    "    print(\"\\n\", metric)\n",
    "    for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "        betweenness_dict = nx.betweenness_centrality(G, normalized=True, weight=metric, endpoints=False, seed=None)\n",
    "        print(\"\\t\", title, round(betweenness_dict[SOURCE], 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics =  [None, 'inverse_trips', 'geo_distance']\n",
    "\n",
    "for metric in metrics:\n",
    "   \n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(2,5, figsize=(14, 6), sharey=True, sharex=True)\n",
    "    fig.suptitle(f'Betweenness centrality {metric}', size=16)\n",
    "    row, column = 0, 0\n",
    "\n",
    "    for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "\n",
    "        # Row and columns\n",
    "        if covid_phase > 4:\n",
    "            row = 1\n",
    "        if covid_phase == 5:\n",
    "            column = 0\n",
    "        \n",
    "        # Compute closeness\n",
    "        betweenness_dict = nx.betweenness_centrality(G, normalized=True, weight=metric, endpoints=False, seed=None)\n",
    "        df_bet = pd.DataFrame(betweenness_dict.values(), columns=['closeness']).reset_index()\n",
    "        \n",
    "        # Plot histogram\n",
    "        df_bet['closeness'].hist(ax=axs[row][column])\n",
    "        \n",
    "        # Plot mean data vertical line\n",
    "        mean = df_bet['closeness'].mean()\n",
    "        axs[row][column].axvline(x=mean, color='b')\n",
    "        \n",
    "        # Title\n",
    "        axs[row][column].set_title(title, size=8)\n",
    "        axs[row][column].annotate(xycoords='axes fraction', xy=(0.53, 0.9), text=f\"mean: {str(round(mean, 5))}\", size=7.5)\n",
    "\n",
    "        # Update column\n",
    "        column+=1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9. Transitivity and Clustering Coefficient ($C_{ij}$) <a class=\"anchor\" id=\"clustering\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clustering(G, metric, weighted=True, compute_nodes_clustering=False):\n",
    "    \"\"\"\n",
    "    Computes transitivity, clustering and average clustering\n",
    "    \"\"\"\n",
    "    # Fraction of all possible triangles present in G\n",
    "    transitivity = nx.transitivity(G)\n",
    "    \n",
    "    if weighted:\n",
    "        if compute_nodes_clustering:\n",
    "            # Compute the clustering coefficient for nodes\n",
    "            nodes_clustering = nx.clustering(G, weight=metric)\n",
    "        \n",
    "        # Average clustering\n",
    "        average_clustering = nx.average_clustering(G, weight=metric, count_zeros=True)\n",
    "        print(f\"Weighted average clustering: {round(average_clustering,3)}\\n\")\n",
    "    \n",
    "    else:\n",
    "        if compute_nodes_clustering:\n",
    "            # Compute the clustering coefficient for nodes\n",
    "            nodes_clustering = nx.clustering(G)\n",
    "        \n",
    "        # Average clustering\n",
    "        average_clustering = nx.average_clustering(G, count_zeros=True)\n",
    "        print(f\"Average clustering: {round(average_clustering,3)}\\n\")\n",
    "        \n",
    "    return transitivity, nodes_clustering, average_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['trips', 'geo_distance']\n",
    "\n",
    "for metric in metrics:\n",
    "    for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "        print(title)\n",
    "        file = f'{save_url}/{phases_names[covid_phase]}_dict_nodes_clustering_{metric}.pkl'\n",
    "\n",
    "        if exists(file):\n",
    "            continue\n",
    "        else:\n",
    "            transitivity, nodes_clustering, average_clustering = compute_clustering(\n",
    "                G, metric, weighted=True, compute_nodes_clustering=True)\n",
    "            with open(file,\"wb\") as f:\n",
    "                pickle.dump(nodes_clustering, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clustering\n",
    "metrics = ['trips', 'geo_distance']\n",
    "\n",
    "for metric in metrics: \n",
    "\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(2,5, figsize=(14, 6), sharey=True, sharex=True)\n",
    "    fig.suptitle(f'Clustering coefficient {metric}', size=16)\n",
    "    row, column = 0, 0\n",
    "\n",
    "    for covid_phase, (G, title) in enumerate(zip(list_G, list_titles)):\n",
    "        # Transitivity\n",
    "        print(f\"Transitivity {metric}\")\n",
    "        transitivity = round(nx.transitivity(G),2 )\n",
    "        print(title, transitivity)\n",
    "        \n",
    "        # Clustering plot\n",
    "        # Row and columns\n",
    "        if covid_phase > 4:\n",
    "            row = 1\n",
    "        if covid_phase == 5:\n",
    "            column = 0\n",
    "\n",
    "        # Open data and plot it\n",
    "        with open(f'{save_url}/{phases_names[covid_phase]}_dict_nodes_clustering_{metric}.pkl', 'rb') as handle:\n",
    "            b = pickle.load(handle)\n",
    "        df = pd.DataFrame(list(b.values()), columns=['clustering'])\n",
    "        df.hist(ax=axs[row][column])\n",
    "\n",
    "        # Plot mean data vertical line\n",
    "        mean = df['clustering'].mean()\n",
    "        axs[row][column].axvline(x=mean, color='b')\n",
    "\n",
    "        # Title\n",
    "        axs[row][column].set_title(title, size=8)\n",
    "        axs[row][column].annotate(\n",
    "            xycoords='axes fraction', xy=(0.53, 0.9), text=f\"mean: {str(round(mean, 2))}\", size=7.5)\n",
    "\n",
    "        # Update column\n",
    "        column+=1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (Eurecat)",
   "language": "python",
   "name": "eurecat-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
